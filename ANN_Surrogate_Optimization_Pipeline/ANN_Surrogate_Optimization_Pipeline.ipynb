{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "# License Notice\n",
  "\n",
  "CFD-ANN-PSO-Net: A Computational Framework for Engineering Design Optimization  \n",
  "Copyright (c) 2025 Shaswat Pathak  \n",
  "\n",
  "This code is licensed under the MIT License. You may use, copy, modify, and distribute this code for research and educational purposes, provided that proper credit is given to the author.  \n",
  "\n",
  "Permission is hereby granted, free of charge, to any person obtaining a copy of this code and associated documentation files (the \"Code\"), to deal in the Code without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Code, and to permit persons to whom the Code is furnished to do so, subject to the following conditions:  \n",
  "\n",
  "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Code.  \n",
  "\n",
  "THE CODE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE CODE OR THE USE OR OTHER DEALINGS IN THE CODE.  "
 ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67717fc6-c87f-4c5b-b92c-f4a8365bc3cc",
   "metadata": {},
   "source": [
    "# Deep Learning to calculate T$_{GPU}$ and C$_{total}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8068299-9a15-42db-b65f-5be22913e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# FUNCTIONS FOR MODEL BUILDING, TRAINING, AND EVALUATION\n",
    "################################################################\n",
    "\n",
    "################################################################\n",
    "# Loading necessary libraries\n",
    "################################################################\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer, r2_score, root_mean_squared_error, mean_absolute_error, mean_squared_error, explained_variance_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import os\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import sys\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import json\n",
    "\n",
    "max_processors = os.cpu_count()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "\n",
    "GREEN_TEXT = \"\\033[92m\"\n",
    "BLUE_TEXT = \"\\033[94m\" \n",
    "RED_TEXT = \"\\033[91m\"\n",
    "RESET_TEXT = \"\\033[0m\"\n",
    "\n",
    "################################################################\n",
    "# Loading Input Data\n",
    "################################################################\n",
    "\n",
    "\n",
    "def get_file_from_user():\n",
    "    supported_extensions = ['.csv', '.xlsx', '.xls', '.txt']\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"Do you want to load the file from your home directory? (y/n): \").strip().lower()\n",
    "    \n",
    "        if choice in ['y', 'yes']:\n",
    "            directory = os.path.expanduser(\"~\") + \"/\"\n",
    "            break\n",
    "        elif choice in ['n', 'no']:\n",
    "            directory = input(\"Enter the full path to the file (use '/' or '\\\\\\\\' as needed): \").strip()\n",
    "            if not directory.endswith(('/', '\\\\')):\n",
    "                directory += '/'\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 'y' or 'n'.\")\n",
    "\n",
    "    while True:\n",
    "        file_name = input(\"Enter the file name *including extension* (e.g., data.xls, data.xlsx, input.csv, test.txt): \").strip()\n",
    "\n",
    "        if not any(file_name.lower().endswith(ext) for ext in supported_extensions):\n",
    "            print(f\"{RED_TEXT} Invalid file extension. Supported formats: {', '.join(supported_extensions)}{RESET_TEXT}\")\n",
    "            continue\n",
    "\n",
    "        full_path = os.path.join(directory, file_name)\n",
    "\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"{RED_TEXT} Error: File '{file_name}' does not exist at '{directory}'{RESET_TEXT}\")\n",
    "        else:\n",
    "            print(f\"Loading file from: {full_path}\")\n",
    "            return full_path\n",
    "\n",
    "def load_data(file_path):\n",
    "    ext = file_path.lower().split('.')[-1]\n",
    "\n",
    "    try:\n",
    "        if ext == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif ext in ['xls', 'xlsx']:\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif ext == 'txt':\n",
    "            df = pd.read_csv(file_path, delimiter='\\t', engine='python')\n",
    "        else:\n",
    "            print(f\"{RED_TEXT}Unsupported file format: .{ext}{RESET_TEXT}\")\n",
    "            return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"{RED_TEXT} Failed to load file: {e}{RESET_TEXT}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Check for missing values\n",
    "    if df.isnull().values.any():\n",
    "        print(f\"{RED_TEXT}Error: The file '{file_path}' contains missing values. Exiting the program.{RESET_TEXT}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"{GREEN_TEXT} File loaded successfully. No missing values found.{RESET_TEXT}\")\n",
    "\n",
    "    # Show columns\n",
    "    print(\"\\n Current columns in the dataset:\")\n",
    "    print(*df.columns.tolist(), sep=\", \")\n",
    "\n",
    "    # Ask if user wants to drop any columns\n",
    "    drop_choice = input(\"\\nDo you want to drop any columns before selecting inputs/outputs? (y/n): \").strip().lower()\n",
    "    while drop_choice not in ['y', 'n']:\n",
    "        print(f\"{RED_TEXT}Invalid input. Please enter 'y' or 'n'.{RESET_TEXT}\")\n",
    "        drop_choice = input(\"Do you want to drop any columns? (y/n): \").strip().lower()\n",
    "\n",
    "    if drop_choice == 'y':\n",
    "        while True:\n",
    "            try:\n",
    "                num_cols_to_drop = int(input(\"How many columns do you want to drop (e.g., 1, 2, 3 .... more): \").strip())\n",
    "                if num_cols_to_drop <= 0:\n",
    "                    raise ValueError\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(f\"{RED_TEXT}Invalid input. Please enter a positive integer.{RESET_TEXT}\")\n",
    "\n",
    "        cols_to_drop = []\n",
    "        for i in range(num_cols_to_drop):\n",
    "            while True:\n",
    "                col_name = input(f\"Enter the name of column {i+1} to drop: \").strip()\n",
    "                if col_name in df.columns:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"{RED_TEXT}Error: Column '{col_name}' not found. Please enter a valid column name.{RESET_TEXT}\")\n",
    "\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        print(f\"{GREEN_TEXT}Successfully dropped columns: {cols_to_drop}{RESET_TEXT}\")\n",
    "\n",
    "    # After dropping, ask user for input and output column names\n",
    "    print(f\"{GREEN_TEXT}\\n Updated columns:{RESET_TEXT}\")\n",
    "    print(*df.columns.tolist(), sep=\", \")\n",
    "    \n",
    "    # Input columns\n",
    "    while True:\n",
    "        input_cols = input(\"Enter input (X) column names separated by commas: \").strip().split(',')\n",
    "        input_cols = [col.strip() for col in input_cols]\n",
    "        if all(col in df.columns for col in input_cols):\n",
    "            break\n",
    "        else:\n",
    "            print(f\"{RED_TEXT}One or more input columns are invalid. Try again.{RESET_TEXT}\")\n",
    "\n",
    "    # Output column\n",
    "    while True:\n",
    "        output_col = input(\"Enter the output (y) column name: \").strip()\n",
    "        if output_col in df.columns and output_col not in input_cols:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"{RED_TEXT}Invalid or duplicate output column. Try again.{RESET_TEXT}\")\n",
    "\n",
    "    X_data = df[input_cols]\n",
    "    y_data = df[output_col]\n",
    "    \n",
    "    return X_data, y_data, df\n",
    "    \n",
    "################################################################\n",
    "# Creating arrays form the datafile for data preprocessing.\n",
    "################################################################\n",
    "data_file = get_file_from_user()\n",
    "\n",
    "if data_file:\n",
    "    X, y, NN_df = load_data(data_file)\n",
    "    if NN_df is not None:\n",
    "        print(X.head())\n",
    "Unit = input(\"Enter the unit for the (Y) column (for no unit leave blank)\")\n",
    "##########################################################################\n",
    "# Checking if number of parameters are more than datapoint (underfit)\n",
    "##########################################################################\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        data_points = X.shape[0]  # Number of data points.\n",
    "        num_params = X.shape[1]  # Number of parameters.\n",
    "        \n",
    "        # Check condition if true, the code will exit with an error.\n",
    "        if num_params > data_points:\n",
    "            print(f\"{RED_TEXT}Error: Number of parameters ({num_params}) exceeds the number of data points ({data_points}). Please adjust your input datafile. {RESET_TEXT}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Proceed with further processing if datapoints are more than parameters.\n",
    "        else:\n",
    "            print(f\"{GREEN_TEXT}Data is valid. Proceeding with execution.{RESET_TEXT}\")\n",
    "            break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "################################################################\n",
    "# Determine the test size based on data points\n",
    "################################################################\n",
    "\n",
    "if data_points < 20:\n",
    "    TEST_SIZE = 0.15\n",
    "elif 20 <= data_points < 30:\n",
    "    TEST_SIZE = 0.2\n",
    "elif 30 <= data_points < 40:\n",
    "    TEST_SIZE = 0.25\n",
    "else:\n",
    "    TEST_SIZE = 0.3\n",
    "\n",
    "print(f\"{BLUE_TEXT}Number of data points: {data_points}{RESET_TEXT}\")\n",
    "print(f\"{BLUE_TEXT}Test size selected: {TEST_SIZE}{RESET_TEXT}\")\n",
    "\n",
    "################################################################\n",
    "# Random state for data processing\n",
    "################################################################\n",
    "\n",
    "RANDOM_STATE=42\n",
    "\n",
    "################################################################\n",
    "# File path for saving the models and figures\n",
    "################################################################\n",
    "print(\"\\n\")\n",
    "save_location_choice = input(\"Do you want to save the file in the home directory? (y/n): \").strip().lower()\n",
    "\n",
    "if save_location_choice in ['y', 'yes']:\n",
    "    base_dir = os.path.expanduser('~')   # just home directory\n",
    "elif save_location_choice in ['n', 'no']:\n",
    "    while True:\n",
    "        base_dir = input(\"Enter full directory path where you want to save the file: \").strip()\n",
    "        if not os.path.exists(base_dir):\n",
    "            print(f\"{RED_TEXT}Directory does not exist! Please provide a valid path.{RESET_TEXT}\")\n",
    "        elif not os.path.isdir(base_dir):\n",
    "            print(f\"{RED_TEXT}Path exists but is not a directory. Try again.{RESET_TEXT}\")\n",
    "        else:\n",
    "            break\n",
    "else:\n",
    "    print(\"Invalid choice. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Create DNN folder inside base_dir\n",
    "save_dir = os.path.join(base_dir, \"DNN\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"{GREEN_TEXT}Directory created or already exists at: {save_dir}{RESET_TEXT}\")\n",
    "\n",
    "# Prompt for valid filename\n",
    "while True:\n",
    "    savename = input(\"Enter the name to save the results in Excel file (without .xlsx extension): \").strip()\n",
    "    if not savename:\n",
    "        print(f\"{RED_TEXT}Filename cannot be empty. Please enter a valid name.{RESET_TEXT}\")\n",
    "    elif any(char in savename for char in r'\\/:*?\"<>|'):\n",
    "        print(f\"{RED_TEXT}Invalid characters in filename! Avoid \\\\ / : * ? \\\" < > |{RESET_TEXT}\")\n",
    "    else:\n",
    "        full_path = os.path.join(save_dir, savename + \".xlsx\")\n",
    "        if os.path.exists(full_path):\n",
    "            overwrite = input(f\"{YELLOW_TEXT}File '{savename}.xlsx' already exists in 'DNN'. Overwrite? (y/n): {RESET_TEXT}\").strip().lower()\n",
    "            if overwrite == 'y':\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please choose a different filename.\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"{GREEN_TEXT}File will be saved at: {full_path}{RESET_TEXT}\")\n",
    "start_time_total = time.time()\n",
    "print(\"\\n\")\n",
    "################################################################\n",
    "# Function to normalize the Input data using MinMaxScaler\n",
    "# Set range to (0, 1) for easy convergence\n",
    "################################################################\n",
    "\n",
    "def scale_data_x(X):\n",
    "    scaler_X_NN = StandardScaler()\n",
    "    X_scaled = scaler_X_NN.fit_transform(X)\n",
    "    return X_scaled, scaler_X_NN\n",
    "\n",
    "################################################################\n",
    "# Function to normalize the Output data using MinMaxScaler\n",
    "# Set range to (0, 1) for easy convergence\n",
    "################################################################\n",
    "\n",
    "def scale_data_y(y):\n",
    "    scaler_y_NN = StandardScaler()\n",
    "    y_scaled = scaler_y_NN.fit_transform(y.values.reshape(-1, 1))\n",
    "    return y_scaled, scaler_y_NN\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Scaling data\n",
    "################################################################\n",
    "\n",
    "X_scaled, scaler_X_NN = scale_data_x(X)\n",
    "y_scaled, scaler_y_NN = scale_data_y(y)\n",
    "\n",
    "######################################################################################\n",
    "# Function for splitting training and testing data based on predetermined test size\n",
    "######################################################################################\n",
    "\n",
    "def split_data(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "################################################################\n",
    "# Spliting data for training and testing\n",
    "################################################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X_scaled, y_scaled)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Customized loss function for weighted loss\n",
    "################################################################\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    error = y_pred - y_true\n",
    "    mse = np.mean(error ** 2)\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "def mae(y_true, y_pred):\n",
    "    error = y_pred - y_true\n",
    "    return np.mean(np.abs(error))\n",
    "    \n",
    "# R2 score function.\n",
    "def R2(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return r2\n",
    "\n",
    "################################################################\n",
    "# Funtion to calculate total number of hidden layers\n",
    "################################################################\n",
    "\n",
    "def calculate_total_neurons(layers):\n",
    "    return sum(layers)\n",
    "\n",
    "################################################################\n",
    "# User Input for Hyperparameters\n",
    "################################################################\n",
    "\n",
    "while True:\n",
    "    user_choice = input(\"Would you like to manually enter hyperparameters or use automatic tuning with GridSearchCV? (enter 'manual' or 'auto'): \").strip().lower()\n",
    "    \n",
    "    if user_choice == 'manual':\n",
    "        \n",
    "        # Displaying important note before collecting inputs.\n",
    "        print(\"\\033[1mNote: These hyperparameters are defined as per the problem and data complexity.\\033[0m\\n\")\n",
    "        print(\"\\033[1mChanging these may cause variations in results.\\033[0m\\n\")\n",
    "        print(\"\\033[1mALL VALUES ARE CASE SENSITIVE.\\033[0m\\n\")\n",
    "\n",
    "        # Function to validate input as an integer within a specified range.\n",
    "        def get_integer_input(prompt, min_val, max_val, preferred_val=None):\n",
    "            while True:\n",
    "                try:\n",
    "                    value = input(f\"{prompt} (Range: {max_val}-{min_val}, Can try: {preferred_val}): \")\n",
    "                    value = int(value)\n",
    "                    if min_val <= value <= max_val:\n",
    "                        return value\n",
    "                    else:\n",
    "                        print(f\"{RED_TEXT}Please enter a value between {min_val} and {max_val}.{RESET_TEXT}\")\n",
    "                except ValueError:\n",
    "                    print(f\"{RED_TEXT}Invalid input. Please enter an integer.{RESET_TEXT}\")\n",
    "\n",
    "        # Function to validate input as a float within a specified range.\n",
    "        def get_float_input(prompt, min_val, max_val, preferred_val=None):\n",
    "            while True:\n",
    "                try:\n",
    "                    value = input(f\"{prompt} (Range: {max_val}-{min_val}, Can try: {preferred_val}): \")\n",
    "                    value = float(value)\n",
    "                    if min_val <= value <= max_val:\n",
    "                        return value\n",
    "                    else:\n",
    "                        print(f\"{RED_TEXT}Please enter a value between {min_val} and {max_val}.{RESET_TEXT}\")\n",
    "                except ValueError:\n",
    "                    print(f\"{RED_TEXT}Invalid input. Please enter a float.{RESET_TEXT}\")\n",
    "\n",
    "        # Function for hidden layers.\n",
    "        def get_hidden_layers():\n",
    "            hidden_layers = []\n",
    "            \n",
    "            # First mandatory hidden layer.\n",
    "            first_layer = get_integer_input(\n",
    "                \"Enter number of neurons for the first hidden layer (Can try: 128)\", \n",
    "                4, 4096, preferred_val=128\n",
    "            )\n",
    "            hidden_layers.append(first_layer)\n",
    "            \n",
    "            # Optional second and third layers..\n",
    "            print(\"Enter number of neurons for the second and third hidden layers in descending order (multiples of 2, max 4096, ending at any multiple of 2 except 2). Each layer is optional.\")\n",
    "            \n",
    "            preferred_layers = [64, 32]  # Preferred values for the second and third layers.\n",
    "            for i in range(2):  # Loop for the next two layers.\n",
    "                while True:\n",
    "                    cont = input(f\"Do you want to add layer {i+2} (Can try: {preferred_layers[i]} if available)? (y/n): \").strip().lower()\n",
    "                    if cont not in ['y', 'n']:\n",
    "                        print(f\"{RED_TEXT}Invalid input. Please enter 'y' or 'n'.{RESET_TEXT}\")\n",
    "                    else:\n",
    "                        break\n",
    "                if cont == 'n':\n",
    "                    break\n",
    "                while True:\n",
    "                    layer = get_integer_input(\n",
    "                        f\"Enter number of neurons for layer {i+2}\",\n",
    "                        4, 4096, preferred_val=preferred_layers[i]\n",
    "                    )\n",
    "                    if layer == 2:\n",
    "                        print(f\"{RED_TEXT}Number of neurons in hidden-layer cannot be 2. Please enter a different value.{RESET_TEXT}\")\n",
    "                    elif layer >= hidden_layers[-1]:\n",
    "                        print(f\"{RED_TEXT}Please enter a smaller value than the previous layer ({hidden_layers[-1]}).{RESET_TEXT}\")\n",
    "                    else:\n",
    "                        hidden_layers.append(layer)\n",
    "                        break\n",
    "\n",
    "            # Additional layers beyond the third layer.\n",
    "            if len(hidden_layers) >= 3:\n",
    "                while True:\n",
    "                    while True:\n",
    "                        cont = input(\"Do you want to add another layer? (y/n): \").strip().lower()\n",
    "                        if cont not in ['y', 'n']:\n",
    "                            print(f\"{RED_TEXT}Invalid input. Please enter 'y' or 'n'.{RESET_TEXT}\")\n",
    "                        else:\n",
    "                            break\n",
    "                    if cont == 'n':\n",
    "                        break\n",
    "                    while True:\n",
    "                        layer = get_integer_input(\"Enter number of neurons for the layer: \", 4, 4096)\n",
    "                        if layer == 2:\n",
    "                            print(f\"{RED_TEXT}Number of neurons in hidden-layer cannot be 2. Please enter a different value.{RESET_TEXT}\")\n",
    "                        elif layer >= hidden_layers[-1]:\n",
    "                            print(f\"{RED_TEXT}Please enter a smaller value than the previous layer ({hidden_layers[-1]}).{RESET_TEXT}\")\n",
    "                        else:\n",
    "                            hidden_layers.append(layer)\n",
    "                            break\n",
    "            return hidden_layers\n",
    "\n",
    "        # Collecting other parameters from user.\n",
    "        \n",
    "        LEARNING_RATE = get_float_input(\"Enter the learning rate\", 0.00001, 0.5, preferred_val=0.01)\n",
    "        NUM_EPOCHS = get_integer_input(\"Enter the number of epochs (iterations)\", 50, 1000, preferred_val=100)\n",
    "        BATCH_SIZE = get_integer_input(\"Enter the batch size\", 2, 512, preferred_val=2)\n",
    "        RANDOM_STATE = get_integer_input(\"Enter the random state\", 0, 314, preferred_val=42)\n",
    "        \n",
    "        # Finalize hidden layers based on user input.\n",
    "        HIDDEN_LAYERS = get_hidden_layers()\n",
    "\n",
    "        # Display the chosen hyperparameters.\n",
    "        print(f\"{BLUE_TEXT} \\n Chosen Hyperparameters: {RESET_TEXT}\")\n",
    "        print(f\"{BLUE_TEXT} Hidden Layers With Neurons: {HIDDEN_LAYERS} {RESET_TEXT}\")\n",
    "        print(f\"{BLUE_TEXT} Learning Rate: {LEARNING_RATE} {RESET_TEXT}\")\n",
    "        print(f\"{BLUE_TEXT} Number of Epochs (iterations): {NUM_EPOCHS} {RESET_TEXT}\")\n",
    "        print(f\"{BLUE_TEXT} Batch Size: {BATCH_SIZE} {RESET_TEXT}\")\n",
    "        print(f\"{BLUE_TEXT} Test Size: {TEST_SIZE} {RESET_TEXT}\")\n",
    "        print(\"\\n\")\n",
    "        break\n",
    "\n",
    "    elif user_choice == 'auto':\n",
    "        \n",
    "        # Determining the number of processor for grid search.\n",
    "        while True:\n",
    "            try:\n",
    "                num_processors = int(input(\"Input the number of CPU processors you want to use for parallel processing (For all use -1): \"))\n",
    "                if num_processors == -1 or (num_processors > 0 and num_processors <= max_processors):\n",
    "                    break\n",
    "                else:\n",
    "                    if num_processors > max_processors:\n",
    "                        print(f\"{RED_TEXT}Error: The number exceeds the available processors ({max_processors}).{RESET_TEXT}\")\n",
    "                    else:\n",
    "                        print(f\"{RED_TEXT}Error: Please enter -1 or an integer greater than 0.{RESET_TEXT}\")\n",
    "            except ValueError:\n",
    "                print(f\"{RED_TEXT}Error: Invalid input. Please enter an integer.{RESET_TEXT}\")\n",
    "            \n",
    "        print(f\"{GREEN_TEXT}\\nAutomatically selecting NN hyperparameters with GridSearchCV...{RESET_TEXT}\")\n",
    "\n",
    "        #######################\n",
    "        # GridSearchCV Setup.\n",
    "        #######################\n",
    "        class TrackedMLP(BaseEstimator, RegressorMixin):\n",
    "            def __init__(self, **kwargs):\n",
    "                self.model = MLPRegressor(**kwargs)\n",
    "                self.n_iter_ = None\n",
    "                self.loss_curve_ = None\n",
    "    \n",
    "            def fit(self, X, y):\n",
    "                self.model.fit(X, y)\n",
    "                self.n_iter_ = self.model.n_iter_\n",
    "                self.loss_curve_ = self.model.loss_curve_\n",
    "                return self\n",
    "    \n",
    "            def predict(self, X):\n",
    "                return self.model.predict(X)\n",
    "    \n",
    "            def get_params(self, deep=True):\n",
    "                return self.model.get_params(deep)\n",
    "    \n",
    "            def set_params(self, **params):\n",
    "                self.model.set_params(**params)\n",
    "                return self\n",
    "        \n",
    "        scorings = {\n",
    "            'MAE': make_scorer(mae, greater_is_better=False),\n",
    "            'RMSE': make_scorer(root_mean_squared_error, greater_is_better=False),\n",
    "            'R2': make_scorer(R2, greater_is_better=True),\n",
    "        }\n",
    "        \n",
    "        #######################\n",
    "        # Parameter Grid Blocks\n",
    "        #######################\n",
    "        \n",
    "        # Learning rates: log-spaced from 1e-1 to 1e-6\n",
    "        learning_rates = np.logspace(-1, -3, num=3).tolist()\n",
    "\n",
    "        # Hidden layer architectures grouped by complexity\n",
    "        hidden_layer_blocks = {\n",
    "            \"shallow\": [\n",
    "                (32,), (64,), (128,), (256,),             # single-layer\n",
    "                (32, 16), (64, 32), (128, 64),            # 2-layer shallow\n",
    "                (32, 16, 8), (64, 32, 16), (128, 64, 32)  # 3-layer shallow\n",
    "            ]\n",
    "\n",
    "            \"medium\": [\n",
    "                (512,), (1024,),\n",
    "                (64, 32), (128, 64), (256, 128)\n",
    "            ],\n",
    "            \"deep\": [\n",
    "                (512, 256), (128, 64, 32), (256, 128, 64),\n",
    "                (512, 256, 128), (256, 128, 64, 32)\n",
    "            ],\n",
    "            \"very_deep\": [\n",
    "                (512, 256, 128, 64), \n",
    "                (512, 256, 128, 64, 32), \n",
    "                (512, 256, 128, 64, 32, 16),\n",
    "                (512, 256, 128, 64, 32, 16, 8)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Batch size groups\n",
    "        batch_small  = [2, 4, 8, 16]\n",
    "\n",
    "        # Epochs by depth\n",
    "        epochs_shallow   = [100, 200, 300, 400, 500]\n",
    "        epochs_medium    = [600, 800]\n",
    "        epochs_deep      = [1000, 1500]\n",
    "\n",
    "        # Final single param_grid with blocks\n",
    "        param_grid = [\n",
    "            {   # Shallow nets: high LR, small/medium batch, fewer epochs\n",
    "            'hidden_layer_sizes': hidden_layer_blocks[\"shallow\"],\n",
    "            'learning_rate_init': learning_rates,\n",
    "            'batch_size': batch_small + batch_medium,\n",
    "            'max_iter': epochs_shallow + epochs_medium + epochs_deep\n",
    "        },\n",
    "        {   # Medium nets: moderate LR, medium batch, moderate epochs\n",
    "            'hidden_layer_sizes': hidden_layer_blocks[\"medium\"],\n",
    "            'learning_rate_init': learning_rates,\n",
    "            'batch_size': batch_small + batch_medium,\n",
    "            'max_iter': epochs_shallow + epochs_medium\n",
    "        },\n",
    "        {   # Deep nets: low LR, medium/large batch, longer epochs\n",
    "            'hidden_layer_sizes': hidden_layer_blocks[\"deep\"],\n",
    "            'learning_rate_init': learning_rates,\n",
    "            'batch_size': batch_small + batch_medium,\n",
    "            'max_iter': epochs_shallow + epochs_medium \n",
    "        },\n",
    "        {   # Very deep nets: tiny LR, large batch, max epochs\n",
    "            'hidden_layer_sizes': hidden_layer_blocks[\"very_deep\"],\n",
    "            'learning_rate_init': learning_rates,\n",
    "            'batch_size': batch_small + batch_medium,\n",
    "            'max_iter': epochs_shallow \n",
    "        }\n",
    "        ]\n",
    "        ##############################################\n",
    "        # Model Setup for MLP regressor.\n",
    "        ##############################################\n",
    "        \n",
    "        model_1 = TrackedMLP(random_state = RANDOM_STATE, early_stopping=False)\n",
    "        \n",
    "        ##############################################\n",
    "        # Performing Grid Search CV for each block.\n",
    "        ##############################################\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_1, \n",
    "            param_grid=param_grid, \n",
    "            scoring=scorings,  # Use custom scoring function\n",
    "            refit=\"MAE\",\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "            n_jobs=num_processors, \n",
    "            verbose=3,\n",
    "            return_train_score=True\n",
    "            )\n",
    "\n",
    "        # Progress Tracking for GridSearchCV.\n",
    "        print(\"Starting Grid Search...\")\n",
    "        \n",
    "        grid_search.fit(X_scaled, y_scaled.ravel())\n",
    "\n",
    "        # Extract best model (if early-stopped)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Actual early-stopped iteration count and loss curve from wrapped model\n",
    "        if hasattr(best_model, \"n_iter_\"):\n",
    "            actual_iter = best_model.n_iter_\n",
    "            print(f\"\\n Best model stopped after {actual_iter} iterations.\")\n",
    "\n",
    "        if hasattr(best_model, \"loss_curve_\"):\n",
    "            final_loss = best_model.loss_curve_[-1] if best_model.loss_curve_ else None\n",
    "            print(f\" Final training loss after early stopping: {final_loss:.6f}\")\n",
    "        \n",
    "        \n",
    "        joblib.dump(grid_search, os.path.join(save_file_path, 'grid_search_checkpoint.pkl'))\n",
    "        \n",
    "        cv_results = grid_search.cv_results_\n",
    "        \n",
    "        ###############################################################\n",
    "        # Saving results to dataframe and filterring overfitting\n",
    "        ###############################################################\n",
    "        # Access the complete results in DataFrame format and Saved results for further analysis.\n",
    "        # List of metrics to correct in Excel file, as grid search include -ve sign infront of minimization metrices.\n",
    "        results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "        results_df['best_n_iter'] = actual_iter\n",
    "        results_df['best_final_loss'] = final_loss\n",
    "        \n",
    "        # Overfitting threshold \n",
    "        overfit_threshold = 0.07\n",
    "\n",
    "        results_df['train_test_gap'] = results_df['mean_train_R2'] - results_df['mean_test_R2']\n",
    "\n",
    "        # Filter non-overfitted results\n",
    "        filtered_results_df = results_df[results_df['train_test_gap'] <= overfit_threshold]\n",
    "\n",
    "        if filtered_results_df.empty:\n",
    "            print(\"All models appear overfitted. Using original best model from GridSearchCV.\")\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_params_1 = grid_search.best_params_\n",
    "            best_loss = grid_search.best_score_\n",
    "        \n",
    "        else:\n",
    "            # Pick best from filtered (non-overfit) results\n",
    "            best_row = filtered_results_df.sort_values(by='mean_test_MAE', ascending=False).iloc[0]\n",
    "            best_params_1 = best_row['params']\n",
    "            best_loss = best_row['mean_test_MAE']\n",
    "\n",
    "        \n",
    "        # Save the best parameters to specific variables.\n",
    "        LEARNING_RATE = best_params_1['learning_rate_init']\n",
    "        NUM_EPOCHS = best_params_1['max_iter']\n",
    "        BATCH_SIZE = best_params_1['batch_size']\n",
    "        HIDDEN_LAYERS = best_params_1['hidden_layer_sizes']\n",
    "        \n",
    "        print(f\"{BLUE_TEXT}\\n Best hyperparameters test: {best_params_1}{RESET_TEXT}\")\n",
    "        print(f\"{BLUE_TEXT}\\n Best MAE loss score test: {np.abs(best_loss)}{RESET_TEXT}\")\n",
    "        \n",
    "                        \n",
    "        \n",
    "        # For Examples (columns containing \"MAE\", \"RMSE\").\n",
    "        error_metrics = [col for col in results_df.columns if any(err in col for err in [\"MAE\", \"RMSE\"])]\n",
    "\n",
    "        # Convert negative error values to positive (absolute values).\n",
    "        results_df[error_metrics] = results_df[error_metrics].abs()\n",
    "        results_df.to_excel(os.path.join(save_file_path, f'{savename}_GridSearch_results.xlsx'),index=True)\n",
    "        filtered_results_df.to_excel(os.path.join(save_file_path, f'{savename}_GridSearch_Filtered_NoOverfit.xlsx'), index=False)\n",
    "        pd.DataFrame({'loss': best_model.loss_curve_}).to_excel(os.path.join(save_file_path, f'{savename}_BestModel_LossCurve.xlsx'), index_label='Epoch')\n",
    "        ##################################################################\n",
    "        # Training MLP with best hyperparameter found from Grid Search.\n",
    "        ##################################################################\n",
    "        \n",
    "        start_time_total_MLP = time.time()        \n",
    "        optimized_model = MLPRegressor(\n",
    "            hidden_layer_sizes=HIDDEN_LAYERS,\n",
    "            learning_rate_init=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_iter=NUM_EPOCHS,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        start_time_1_MLP=time.time()\n",
    "        optimized_model.fit(X_train, y_train)\n",
    "        end_time_1_MLP=time.time()\n",
    "        elapsed_time_1_MLP= end_time_1_MLP - start_time_1_MLP\n",
    "        \n",
    "        \n",
    "\n",
    "        start_time_2_MLP=time.time()\n",
    "        y_train_pred_MLP = optimized_model.predict(X_train)\n",
    "        end_time_2_MLP=time.time()\n",
    "        elapsed_time_2_MLP= end_time_2_MLP - start_time_2_MLP\n",
    "        \n",
    "        start_time_3_MLP =time.time()\n",
    "        y_test_pred_MLP = optimized_model.predict(X_test)\n",
    "        end_time_3_MLP = time.time()\n",
    "        elapsed_time_3_MLP = end_time_3_MLP - start_time_3_MLP\n",
    "        \n",
    "        y_actual_train_orig_MLP = scaler_y_NN.inverse_transform(y_train.reshape(1, -1))\n",
    "        y_predicted_train_orig_MLP = scaler_y_NN.inverse_transform(y_train_pred_MLP.reshape(1, -1))\n",
    "        y_actual_test_orig_MLP = scaler_y_NN.inverse_transform(y_test.reshape(1, -1))\n",
    "        y_predicted_test_orig_MLP = scaler_y_NN.inverse_transform(y_test_pred_MLP.reshape(1, -1))\n",
    "\n",
    "        n_train_MLP = y_actual_train_orig_MLP.size\n",
    "        n_test_MLP = y_actual_test_orig_MLP.size\n",
    "        \n",
    "        total_layers_MLP = 1 + len(HIDDEN_LAYERS) + 1  # 1 input layer + hidden layers + 1 output layer.\n",
    "        total_neurons_MLP = calculate_total_neurons(HIDDEN_LAYERS)\n",
    "    \n",
    "        num_iterations_MLP = (n_train_MLP) * NUM_EPOCHS\n",
    "    \n",
    "        # Calculating errors for the model.\n",
    "         \n",
    "        MAE_loss_train_MLP = mae(y_actual_train_orig_MLP, y_predicted_train_orig_MLP)\n",
    "        MAE_loss_test_MLP = mae(y_actual_test_orig_MLP, y_predicted_test_orig_MLP)\n",
    "        rmse_train_MLP = root_mean_squared_error(y_actual_train_orig_MLP, y_predicted_train_orig_MLP)\n",
    "        rmse_test_MLP = root_mean_squared_error(y_actual_test_orig_MLP, y_predicted_test_orig_MLP)\n",
    "        R2_score_train_MLP = R2(y_train, y_train_pred_MLP)\n",
    "        R2_score_test_MLP = R2(y_test, y_test_pred_MLP)\n",
    "        \n",
    "        end_time_total_MLP = time.time()\n",
    "        elapsed_time_total_MLP = end_time_total_MLP - start_time_total_MLP\n",
    "        # Ploting actual vs train graphs for MLP and dynamically compute figure size.\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        # Adjust sizes dynamically\n",
    "        total_points = n_train_MLP + n_test_MLP\n",
    "        min_scale = 0.5 \n",
    "        max_scale = 2.5 \n",
    "        base_size = 80\n",
    "        scale_factor_MLP = np.clip(1000 / (total_points + 1), min_scale, max_scale)\n",
    "        \n",
    "        s_size = 80 * scale_factor_MLP\n",
    "        \n",
    "        plt.scatter(y_actual_train_orig_MLP.flatten(), y_predicted_train_orig_MLP.flatten(), color='red', edgecolor='black', s = s_size,\n",
    "                marker ='o', label=fr'Training Data')\n",
    "        plt.scatter(y_actual_test_orig_MLP.flatten(), y_predicted_test_orig_MLP.flatten(), marker ='^', s = s_size,\n",
    "                    label=fr'Test Data', color='none', edgecolor='black')\n",
    "        plt.plot([min(y), max(y)], [min(y), max(y)], color='black', alpha=0.8, linestyle='--', linewidth='1.3', label=fr'Actual Values')    \n",
    "   \n",
    "        \n",
    "        plt.annotate(fr'R$^2$ Score (Train): {R2_score_train_MLP:.5f}', (0.015, 0.81), \n",
    "                     fontsize=11.5, xycoords='axes fraction')\n",
    "        plt.annotate(fr'R$^2$ Score (Test): {R2_score_test_MLP:.5f}', (0.015, 0.74), \n",
    "                     fontsize=11.5, xycoords='axes fraction')\n",
    "        plt.annotate(fr'MAE (Train): {MAE_loss_train_MLP:.2e} {Unit} (Test): {MAE_loss_test_MLP:.2e} {Unit}', \n",
    "             (0.015, 0.88), fontsize=11.5,  xycoords='axes fraction')\n",
    "        plt.annotate(fr'RMSE (Train): {rmse_train_MLP:.2e} {Unit} (Test): {rmse_test_MLP:.2e} {Unit}', \n",
    "             (0.015, 0.95), fontsize=11.5,  xycoords='axes fraction')\n",
    "        \n",
    "        box_props = dict(boxstyle='round,pad=0.4', edgecolor='black', facecolor='white', alpha=0.1)\n",
    "        textstr = '\\n'.join((\n",
    "            fr'MLP Hyperparameters',\n",
    "            fr'',\n",
    "            fr'Input Dimension: {num_params}',\n",
    "            fr'Total Samples: {n_test_MLP + n_train_MLP}',\n",
    "            fr'Train Samples: {n_train_MLP}',\n",
    "            fr'Test Samples: {n_test_MLP}',\n",
    "            fr'Total NN Layers: {total_layers_MLP}',\n",
    "            fr'Total Neurons: {total_neurons_MLP}',\n",
    "            fr'Learning Rate: {LEARNING_RATE}',\n",
    "            fr'No. of Iterations: {NUM_EPOCHS}',\n",
    "            fr'Random State: {RANDOM_STATE}',\n",
    "            fr'Training time: {elapsed_time_1_MLP:.3f} s',\n",
    "            fr'Inference time (Train): {elapsed_time_2_MLP:.3f} s',\n",
    "            fr'Inference time (Test): {elapsed_time_3_MLP:.3f} s',\n",
    "            fr'Total time taken: {elapsed_time_total_MLP:.3f} s'\n",
    "            ))\n",
    "        plt.annotate(textstr, (1.03, 0.20), fontsize=10, xycoords='axes fraction', bbox=box_props)\n",
    "        \n",
    "        \n",
    "        plt.xlabel(fr'Actual Values ({y.name})', fontsize=16, fontweight='bold')\n",
    "        plt.ylabel(f'Predicted Values ({y.name})', fontsize=16, fontweight='bold')\n",
    "        plt.title(fr'{y.name} predictions from MLP', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.legend(loc='lower right', fontsize=12)\n",
    "        plt.grid(False)       \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Saving the figure \n",
    "        plt.savefig(os.path.join(save_file_path, savename + '_MLP_Actual_vs_Prediceted_Data.jpg'), dpi=400)\n",
    "\n",
    "        \n",
    "        # Plot each metric over the course of the iterations (changing hyperparameters).\n",
    "        metrics = ['mean_test_MAE', 'mean_test_RMSE', 'mean_test_R2']\n",
    "        metrics_0 = ['mean_train_MAE','mean_train_RMSE', 'mean_train_R2']\n",
    "        metrics_1 = ['MAE', 'RMSE', 'R2']\n",
    "        metric_labels = ['MAE', 'Root Mean Squared Error (RMSE)', '$R^2$ Score']\n",
    "        metric_labels_1 = ['MAE', 'RMSE', '$R^2$ Score'] \n",
    "        metric_labels_2 = ['MAE','RMS Error', 'R2 Score']\n",
    "        colors = ['lime', 'blue', '#FF4500']\n",
    "        \n",
    "        markers = ['o', 'o', 'o'] \n",
    "        higher_is_better = [False, False, True]\n",
    "        \n",
    "        for metric, metric_0, metric_1, label, label_1, color, marker, is_higher_better, saving_name in zip(metrics, metrics_0, metrics_1, metric_labels, metric_labels_1, colors, markers, higher_is_better, metric_labels_2):\n",
    "            \n",
    "            optimum_score_index = results_df[f'rank_test_{metric_1}'].argmin()  # Index of the best score for each metric.\n",
    "            optimum_params = results_df['params'][optimum_score_index]  # Best params corresponding to that score.\n",
    "            optimum_score = results_df[f'mean_test_{metric_1}'][optimum_score_index]  # Best score for that metric.\n",
    "            \n",
    "            LEARNING_RATE_1 = optimum_params.get('learning_rate_init')\n",
    "            NUM_EPOCHS_1 = optimum_params.get('max_iter')\n",
    "            BATCH_SIZE_1 = optimum_params.get('batch_size')\n",
    "            HIDDEN_LAYERS_1 = optimum_params.get('hidden_layer_sizes')\n",
    "\n",
    "            # Dynamic plot size calculation\n",
    "            base_width, base_height = 18, 6\n",
    "            scale_w = 1 + 0.015 * (total_neurons_MLP // 10)\n",
    "            scale_h = 1 + 0.015 * total_layers_MLP\n",
    "\n",
    "            fig_width = min(14, base_width * scale_w)\n",
    "            fig_height = min(10, base_height * scale_h)\n",
    "            plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "            # Calculate a scale factor relative to base size\n",
    "            scale_factor = (fig_width * fig_height) / (base_width * base_height)\n",
    "\n",
    "            # Adjust sizes dynamically\n",
    "            marker_size = 10 * scale_factor\n",
    "            marker_edge_width = 1.7 * scale_factor\n",
    "            line_width_test = 1.5 * scale_factor\n",
    "            line_width_train = 2 * scale_factor\n",
    "            tick_fontsize = 22 * scale_factor\n",
    "            label_fontsize = 26 * scale_factor\n",
    "            legend_fontsize = 20 * scale_factor\n",
    "            annot_fontsize = 14 * scale_factor\n",
    "            hline_vline_width = 2.2 * scale_factor\n",
    "\n",
    "            # Plotting\n",
    "            plt.plot(results_df.index + 1, results_df[metric],\n",
    "                     marker=marker, linestyle='-.', linewidth=line_width_test, alpha=0.35,\n",
    "                     markersize=marker_size, markeredgecolor='black', markeredgewidth=marker_edge_width,\n",
    "                     markerfacecolor=color, color='black', label='test', zorder=1)\n",
    "\n",
    "            plt.plot(results_df.index + 1, results_df[metric_0],\n",
    "                     marker='v', linestyle='-', linewidth=line_width_train, alpha=0.9,\n",
    "                     markersize=marker_size * 1.1, markeredgecolor='black', markeredgewidth=marker_edge_width * 0.8,\n",
    "                     markerfacecolor='white', color='black', label='train', zorder=0)\n",
    "\n",
    "            # Best value and line indicators\n",
    "            if is_higher_better:\n",
    "                best_index = results_df[metric].idxmax()\n",
    "                best_value = results_df[metric].max()\n",
    "            else:\n",
    "                best_index = results_df[metric].idxmin()\n",
    "                best_value = results_df[metric].min()\n",
    "\n",
    "            plt.axhline(best_value, color='red', linestyle='--', linewidth=hline_vline_width,\n",
    "                        label=f'Best {label}: {best_value:.4f}', zorder=2)\n",
    "            plt.axvline(best_index + 1, color='red', linestyle='--', linewidth=hline_vline_width, zorder=2)\n",
    "\n",
    "\n",
    "            plt.xlabel(\"Iteration from Grid Search CV (Hyperparameter Combination)\", fontsize=label_fontsize)\n",
    "            plt.ylabel(label, fontsize=label_fontsize)\n",
    "            plt.xticks(fontsize=tick_fontsize)\n",
    "            plt.yticks(fontsize=tick_fontsize)\n",
    "\n",
    "            # Legend\n",
    "            if metric_1 in ['MAE', 'RMSE']:\n",
    "                legend_loc = 'upper right'\n",
    "            elif metric_1 in ['R2']:\n",
    "                legend_loc = 'lower right'\n",
    "\n",
    "            # Plot legend\n",
    "            plt.legend(\n",
    "                [f\"{'Test'}\", f\"{'Train'}\", f\"Best {label_1}: {best_value:.4f}\"],\n",
    "                loc=legend_loc,\n",
    "                fontsize=legend_fontsize,\n",
    "                facecolor='white'\n",
    "            )\n",
    "            \n",
    "            # Annotation box\n",
    "            box_props = dict(\n",
    "                boxstyle=f'round,pad={0.5 * scale_factor:.2f}',\n",
    "                edgecolor='black',\n",
    "                facecolor='white',\n",
    "                alpha=0.25\n",
    "            )\n",
    "            if metric_1 in ['MAE', 'RMSE']:\n",
    "                ann_xy = (0.02, 0.97)  \n",
    "                ann_va = 'top'\n",
    "            elif metric_1 in ['R2']:\n",
    "                ann_xy = (0.02, 0.03)  \n",
    "                ann_va = 'bottom'\n",
    "            \n",
    "            textstr = '\\n'.join((\n",
    "                fr'Best Hyperparameters ({label_1})',\n",
    "                fr'',\n",
    "                fr'Learning Rate: {LEARNING_RATE_1}',\n",
    "                fr'Number of Iterations: {NUM_EPOCHS_1}',\n",
    "                fr'Batch Size: {BATCH_SIZE_1}',\n",
    "                fr'Hidden Layer: {HIDDEN_LAYERS_1}'\n",
    "            ))\n",
    "\n",
    "            plt.annotate(\n",
    "                textstr,\n",
    "                xy=ann_xy,  \n",
    "                fontsize=annot_fontsize * 1.25,\n",
    "                xycoords='axes fraction',\n",
    "                bbox=box_props,\n",
    "                verticalalignment=ann_va,\n",
    "                horizontalalignment='left'\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Final layout and save\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(save_file_path, f\"{savename}_{saving_name.replace(' ', '_')}.jpg\"), dpi=400)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # =====================================\n",
    "        # Plot Best Model's 5-Fold CV Results\n",
    "        # =====================================\n",
    "\n",
    "        # Get the index of the best model according to refit metric (MAE in your case)\n",
    "        best_idx = grid_search.best_index_\n",
    "        \n",
    "        # Extract per-fold test scores for R and MAE\n",
    "        # Note: MAE will be negative because greater_is_better=False, so take absolute value\n",
    "        r2_scores = [\n",
    "            cv_results[f'split{i}_test_R2'][best_idx]\n",
    "            for i in range(5)\n",
    "        ]\n",
    "        mae_scores = [\n",
    "            abs(cv_results[f'split{i}_test_MAE'][best_idx])\n",
    "            for i in range(5)\n",
    "        ]\n",
    "\n",
    "        folds = np.arange(1, 6)  # Fold numbers 15\n",
    "        width = 0.35\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(5.5, 4.5))\n",
    "\n",
    "        # Left axis: R\n",
    "        color_r2 = 'darkblue'\n",
    "        ax1.bar(folds - width/2, r2_scores, width,\n",
    "                color=color_r2, edgecolor='black', label=r'$R^2$ Score')\n",
    "        ax1.set_xlabel('Fold Number', fontsize=14)\n",
    "        ax1.set_ylabel(r'$R^2$ Score', color=color_r2, fontsize=14, labelpad=5)\n",
    "        ax1.tick_params(axis='y', labelcolor=color_r2, labelsize=14)\n",
    "        ax1.set_ylim([min(r2_scores)-0.02, 1.01])\n",
    "        ax1.set_xticks(folds)\n",
    "        ax1.set_xticklabels(folds, fontsize=14)\n",
    "        ax1.locator_params(axis='y', nbins=4)\n",
    "\n",
    "        # Right axis: MAE\n",
    "        ax2 = ax1.twinx()\n",
    "        color_mae = 'tab:red'\n",
    "        ax2.bar(folds + width/2, mae_scores, width,\n",
    "                color=color_mae, edgecolor='black', label='MAE')\n",
    "        ax2.set_ylabel(f'MAE ({Unit})', color=color_mae, fontsize=14, labelpad=10)\n",
    "        ax2.tick_params(axis='y', labelcolor=color_mae, labelsize=14)\n",
    "        ax2.set_ylim([0, max(mae_scores)+0.03])\n",
    "        ax2.locator_params(axis='y', nbins=5)\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        lines_labels = [ax.get_legend_handles_labels() for ax in [ax1, ax2]]\n",
    "        lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "        ax1.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, 1.12),\n",
    "                   ncol=2, fontsize=14, frameon=False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_file_path, f\"{savename}_Best5Fold_BarPlot_MLP.jpg\"), dpi=700)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Mean R (best model 5 folds): {:.4f}  {:.4f}\".format(np.mean(r2_scores), np.std(r2_scores)))\n",
    "        print(\"Mean MAE (best model 5 folds): {:.4f}  {:.4f}\".format(np.mean(mae_scores), np.std(mae_scores)))\n",
    "                      \n",
    "        break\n",
    "\n",
    "    else:\n",
    "        print(f\"{RED_TEXT}Invalid input. Please enter 'manual' or 'auto' to continue.{RESET_TEXT}\")\n",
    "\n",
    "print(f\"{GREEN_TEXT} Grid Search completed.{RESET_TEXT}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "################################################################\n",
    "# Deep Neural Network model building\n",
    "################################################################\n",
    "\n",
    "################################################################\n",
    "# Function for DNN model building with optimized hyperparameters\n",
    "################################################################\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First layer\n",
    "    model.add(layers.Dense(\n",
    "        HIDDEN_LAYERS[0], \n",
    "        input_shape=(input_shape,),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        kernel_regularizer=regularizers.l2(0.0001)\n",
    "    ))\n",
    "    model.add(layers.PReLU())\n",
    "\n",
    "    # Hidden layers\n",
    "    for layer_size in HIDDEN_LAYERS[1:]:\n",
    "        model.add(layers.Dense(\n",
    "            layer_size,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(0.0001)\n",
    "        ))\n",
    "        model.add(layers.PReLU())\n",
    "\n",
    "    # Output layer for regression (no activation)\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "################################################################\n",
    "# Compile and train function for DNN\n",
    "################################################################\n",
    "\n",
    "def compile_and_train(model, X_train, y_train, X_test, y_test, \n",
    "                      learning_rate=LEARNING_RATE, num_epochs=NUM_EPOCHS, \n",
    "                      batch_size=BATCH_SIZE, max_wall_time_hours=2):\n",
    "    \"\"\"\n",
    "    Complie and train function with multiple termination conditions:\n",
    "    1. Maximum epoch limit (1500)\n",
    "    2. Validation loss threshold (1e-6) with patience window (5 epochs)\n",
    "    3. Maximum wall-clock time (2 hours)\n",
    "    4. Early stopping based on validation loss improvement\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-5)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Multiple termination callbacks\n",
    "    callbacks_list = []\n",
    "    \n",
    "    # 1. Early stopping for validation loss improvement\n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=1e-4,\n",
    "        patience=40,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(early_stop)\n",
    "    \n",
    "    # 2. Custom callback for validation loss threshold with patience\n",
    "    class ValidationLossThresholdCallback(callbacks.Callback):\n",
    "        def __init__(self, threshold=1e-6, patience=50):\n",
    "            super().__init__()\n",
    "            self.threshold = threshold\n",
    "            self.patience = patience\n",
    "            self.wait = 0\n",
    "            \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            current_val_loss = logs.get('val_loss')\n",
    "            if current_val_loss is not None:\n",
    "                if current_val_loss < self.threshold:\n",
    "                    self.wait += 1\n",
    "                    print(f\"\\nValidation loss {current_val_loss:.2e} below threshold {self.threshold:.2e}. \"\n",
    "                          f\"Patience: {self.wait}/{self.patience}\")\n",
    "                    if self.wait >= self.patience:\n",
    "                        print(f\"\\nTerminating: Validation loss below {self.threshold:.2e} for {self.patience} epochs\")\n",
    "                        self.model.stop_training = True\n",
    "                else:\n",
    "                    self.wait = 0\n",
    "    \n",
    "    val_threshold_callback = ValidationLossThresholdCallback(threshold=1e-3, patience=100)\n",
    "    callbacks_list.append(val_threshold_callback)\n",
    "    \n",
    "    # 3. Custom callback for maximum wall-clock time\n",
    "    class WallTimeCallback(callbacks.Callback):\n",
    "        def __init__(self, max_time_hours=2):\n",
    "            super().__init__()\n",
    "            self.max_time_seconds = max_time_hours * 3600\n",
    "            self.start_time = None\n",
    "            \n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            print(f\"Training started. Maximum wall-clock time: {self.max_time_seconds/3600:.1f} hours\")\n",
    "            \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if self.start_time is not None:\n",
    "                elapsed_time = time.time() - self.start_time\n",
    "                if elapsed_time > self.max_time_seconds:\n",
    "                    print(f\"\\nTerminating: Maximum wall-clock time ({self.max_time_seconds/3600:.1f} hours) exceeded\")\n",
    "                    print(f\"Elapsed time: {elapsed_time/3600:.2f} hours\")\n",
    "                    self.model.stop_training = True\n",
    "    \n",
    "    wall_time_callback = WallTimeCallback(max_time_hours=max_wall_time_hours)\n",
    "    callbacks_list.append(wall_time_callback)\n",
    "    \n",
    "    # 4. Custom callback for maximum epoch limit\n",
    "    class MaxEpochCallback(callbacks.Callback):\n",
    "        def __init__(self, max_epochs=1500):\n",
    "            super().__init__()\n",
    "            self.max_epochs = max_epochs\n",
    "            \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if epoch + 1 >= self.max_epochs:\n",
    "                print(f\"\\nTerminating: Maximum epoch limit ({self.max_epochs}) reached\")\n",
    "                self.model.stop_training = True\n",
    "    \n",
    "    max_epoch_callback = MaxEpochCallback(max_epochs=1500)\n",
    "    callbacks_list.append(max_epoch_callback)\n",
    "    \n",
    "    # Training with all termination conditions\n",
    "    start_time = time.time()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING WITH TERMINATION CONDITIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Termination conditions:\")\n",
    "    print(\"1. Maximum epochs: 1500\")\n",
    "    print(\"2. Validation loss < 1e-6 for 5 consecutive epochs\")\n",
    "    print(\"3. Maximum wall-clock time: 2 hours\")\n",
    "    print(\"4. Early stopping: validation loss improvement (patience=10)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=min(num_epochs, 1500),  # Respect the maximum epoch limit\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        # Report termination reason\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Training time: {elapsed_time:.2f} seconds ({elapsed_time/3600:.2f} hours)\")\n",
    "        print(f\"Total epochs completed: {len(history.history['loss'])}\")\n",
    "        \n",
    "        # Determine termination reason\n",
    "        total_epochs = len(history.history['loss'])\n",
    "        final_val_loss = history.history['val_loss'][-1] if 'val_loss' in history.history else None\n",
    "        \n",
    "        if total_epochs >= 1500:\n",
    "            print(\"Termination reason: Maximum epoch limit (1500) reached\")\n",
    "        elif elapsed_time > max_wall_time_hours * 3600:\n",
    "            print(f\"Termination reason: Maximum wall-clock time ({max_wall_time_hours} hours) exceeded\")\n",
    "        elif final_val_loss is not None and final_val_loss < 1e-6:\n",
    "            print(\"Termination reason: Validation loss threshold (1e-6) reached with patience\")\n",
    "        else:\n",
    "            print(\"Termination reason: Early stopping due to validation loss plateau\")\n",
    "        \n",
    "        print(f\"Final validation loss: {final_val_loss:.2e}\" if final_val_loss else \"No validation loss recorded\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return history, elapsed_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training interrupted due to error: {str(e)}\")\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        return None, elapsed_time\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Function for calculating error in the prediction from DNN\n",
    "################################################################\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, X_train, y_train):\n",
    "    start_time = time.time()\n",
    "    predicted_train = model.predict(X_train, verbose=0)\n",
    "    end_time = time.time()\n",
    "    elapsed_time_2 = end_time - start_time\n",
    "    print(f\"Inference time (Train): {elapsed_time_2:.2f} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predicted_test = model.predict(X_test, verbose=0)\n",
    "    end_time = time.time()\n",
    "    elapsed_time_3 = end_time - start_time\n",
    "    print(f\"Inference time (Test): {elapsed_time_3:.2f} seconds\")\n",
    "    \n",
    "    y_actual_train_orig = scaler_y_NN.inverse_transform(y_train.reshape(1, -1))\n",
    "    y_predicted_train_orig = scaler_y_NN.inverse_transform(predicted_train.reshape(1, -1))\n",
    "    y_actual_test_orig = scaler_y_NN.inverse_transform(y_test.reshape(1, -1))\n",
    "    y_predicted_test_orig = scaler_y_NN.inverse_transform(predicted_test.reshape(1, -1))\n",
    "\n",
    "    \n",
    "    MAE_loss_train_DNN = mae(y_actual_train_orig, y_predicted_train_orig)\n",
    "    MAE_loss_test_DNN = mae(y_actual_test_orig, y_predicted_test_orig)\n",
    "    rmse_train_DNN = root_mean_squared_error(y_actual_train_orig, y_predicted_train_orig)\n",
    "    rmse_test_DNN = root_mean_squared_error(y_actual_test_orig, y_predicted_test_orig)\n",
    "    R2_score_train_DNN = R2(y_train, predicted_train)\n",
    "    R2_score_test_DNN = R2(y_test, predicted_test)\n",
    "    \n",
    "    return elapsed_time_2, elapsed_time_3, MAE_loss_train_DNN, MAE_loss_test_DNN, rmse_train_DNN, rmse_test_DNN, R2_score_train_DNN, R2_score_test_DNN, predicted_train, predicted_test\n",
    "\n",
    "###################################################################################\n",
    "# Function to Save the DNN model for further use (like during optimization)\n",
    "###################################################################################\n",
    "\n",
    "def save_model(model, scaler_X_NN, scaler_y_NN, save_file_path, model_name):\n",
    "    save_folder = os.path.join(save_file_path, savename)\n",
    "    os.makedirs(save_folder, exist_ok=True) \n",
    "    \n",
    "    model_path = os.path.join(save_folder, f\"{savename}_{model_name}.keras\")\n",
    "    model.save(model_path)\n",
    "\n",
    "###################################################################################\n",
    "# Function to obtain prediction from DNN in an excel file with original scale\n",
    "###################################################################################\n",
    "\n",
    "def save_results_to_excel(y_actual_train, predicted_train, y_actual_test, predicted_test, MAE_loss_train_DNN, MAE_loss_test_DNN, rmse_train_DNN, rmse_test_DNN, R2_score_train_DNN, R2_score_test_DNN, target_name, save_file_path, savename, scaler_X_NN, scaler_y_NN):\n",
    "    \n",
    "    y_actual_train_orig = scaler_y_NN.inverse_transform(y_actual_train)\n",
    "    predicted_train_orig = scaler_y_NN.inverse_transform(predicted_train)\n",
    "\n",
    "    results_train_dict = {\n",
    "        'Actual Train Values from DNN': y_actual_train_orig.flatten(),\n",
    "        'Predicted Train Values from DNN': predicted_train_orig.flatten(),\n",
    "    }\n",
    "    results_train_df = pd.DataFrame(results_train_dict)\n",
    "    results_train_df.to_excel(os.path.join(save_file_path, f'{savename}_DNN_prediceted_train_data_{target_name}.xlsx'),index=False)\n",
    "\n",
    "    y_actual_test_orig = scaler_y_NN.inverse_transform(y_actual_test)\n",
    "    predicted_test_orig = scaler_y_NN.inverse_transform(predicted_test)\n",
    "\n",
    "    results_test_dict = {\n",
    "        'Actual Test Values from DNN': y_actual_test_orig.flatten(),\n",
    "        'Predicted Test Values from DNN': predicted_test_orig.flatten()\n",
    "    }\n",
    "    results_test_df = pd.DataFrame(results_test_dict)\n",
    "    results_test_df.to_excel(os.path.join(save_file_path, f'{savename}_DNN_prediceted_test_data_{target_name}.xlsx'),index=False)\n",
    "\n",
    "################################################################\n",
    "# Function for plotting Actual vs Prediction Graph for DNN\n",
    "################################################################\n",
    "\n",
    "def plot_results(elapsed_time_1, elapsed_time_2, elapsed_time_3, elapsed_time_total_DNN, \n",
    "                 X, y_actual_train, y_predicted_train, y_actual_test, y_predicted_test, \n",
    "                 MAE_loss_train_DNN, MAE_loss_test_DNN, rmse_train_DNN, rmse_test_DNN, \n",
    "                 R2_score_train_DNN, R2_score_test_DNN, \n",
    "                 scaler_X_NN, scaler_y_NN, target_name, savename):\n",
    "\n",
    "    # Inverse transform\n",
    "    y_actual_train_orig = scaler_y_NN.inverse_transform(y_actual_train)\n",
    "    y_predicted_train_orig = scaler_y_NN.inverse_transform(y_predicted_train)\n",
    "    y_actual_test_orig = scaler_y_NN.inverse_transform(y_actual_test)\n",
    "    y_predicted_test_orig = scaler_y_NN.inverse_transform(y_predicted_test)\n",
    "\n",
    "    # Dynamic plot sizing based on sample size\n",
    "    n_train = y_actual_train.size\n",
    "    n_test = y_actual_test.size\n",
    "    total_layers = 1 + len(HIDDEN_LAYERS) + 1\n",
    "    total_neurons = calculate_total_neurons(HIDDEN_LAYERS)\n",
    "    num_iterations = n_train * NUM_EPOCHS\n",
    "\n",
    "    # Dynamic figsize logic\n",
    "   \n",
    "    plt.figure(figsize=(8,5))\n",
    "\n",
    "    # Adjust sizes dynamically\n",
    "    total_layers_DNN = n_train + n_test\n",
    "    min_scale = 0.5 \n",
    "    max_scale = 2.5 \n",
    "    base_size = 80\n",
    "    scale_factor_DNN = np.clip(1000 / (total_layers_DNN + 1), min_scale, max_scale)\n",
    "        \n",
    "    # scale_factor_DNN = 1 / np.log10(total_layers_DNN + 10)\n",
    "    s_size_DNN = 80 * scale_factor_DNN\n",
    "    # Error metrics and annotations\n",
    "    plt.annotate(fr'R$^2$ Score (Train): {R2_score_train_DNN:.5f}', (0.015, 0.81), fontsize=11.5, xycoords='axes fraction')\n",
    "    plt.annotate(fr'R$^2$ Score (Test): {R2_score_test_DNN:.5f}', (0.015, 0.74), fontsize=11.5, xycoords='axes fraction')\n",
    "    plt.annotate(fr'MAE (Train): {MAE_loss_train_DNN:.2e} {Unit} (Test): {MAE_loss_test_DNN:.2e} {Unit}', (0.015, 0.88), fontsize=11.5, xycoords='axes fraction')\n",
    "    plt.annotate(fr'RMSE (Train): {rmse_train_DNN:.2e} {Unit} (Test): {rmse_test_DNN:.2e} {Unit}', (0.015, 0.95), fontsize=11.5, xycoords='axes fraction')\n",
    "\n",
    "    # Scatter plots\n",
    "    plt.scatter(y_actual_train_orig.flatten(), y_predicted_train_orig.flatten(), color='royalblue', edgecolor='black', s = s_size_DNN,\n",
    "                marker='o', label='Training Data')\n",
    "    plt.scatter(y_actual_test_orig.flatten(), y_predicted_test_orig.flatten(), marker='^', s = s_size_DNN,\n",
    "                label='Test Data', color='none', edgecolor='black')\n",
    "    plt.plot([min(y), max(y)], [min(y), max(y)], color='black', alpha=0.8, linestyle='--', linewidth=1.7, label='Actual Values')\n",
    "\n",
    "    # Box with model parameters\n",
    "    box_props = dict(boxstyle='round,pad=0.4', edgecolor='black', facecolor='white', alpha=0.1)\n",
    "    textstr = '\\n'.join((\n",
    "        'DNN Hyperparameters',\n",
    "        '',\n",
    "        fr'Input Dimension: {num_params}',\n",
    "        fr'Total Samples: {n_test + n_train}',\n",
    "        fr'Train Samples: {n_train}',\n",
    "        fr'Test Samples: {n_test}',\n",
    "        fr'Total NN Layers: {total_layers}',\n",
    "        fr'Total Neurons: {total_neurons}',\n",
    "        fr'Learning Rate: {LEARNING_RATE}',\n",
    "        fr'No. of Iterations: {NUM_EPOCHS}',\n",
    "        fr'Random State: {RANDOM_STATE}',\n",
    "        fr'Training time: {elapsed_time_1:.3f} s',\n",
    "        fr'Inference time (Train): {elapsed_time_2:.3f} s',\n",
    "        fr'Inference time (Test): {elapsed_time_3:.3f} s',\n",
    "        fr'Total time taken: {elapsed_time_total_DNN:.3f} s'\n",
    "    ))\n",
    "    plt.annotate(textstr, (1.03, 0.20), fontsize=10, xycoords='axes fraction', bbox=box_props)\n",
    "\n",
    "    plt.xlabel(f'Actual Values ({y.name})', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel(f'Predicted Values ({y.name})', fontsize=16, fontweight='bold')\n",
    "    plt.title(f'{y.name} predictions from DNN with PReLU activation', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(save_file_path, savename + '_DNN_Actual_vs_Prediceted_Data.jpg'), dpi=400)\n",
    "\n",
    "################################################################\n",
    "# Function to plot the errors with iterations with Adam in DNN\n",
    "################################################################\n",
    "def plot_loss(history, savename, termination_info=None):\n",
    "       \n",
    "    if history is None:\n",
    "        print(\"No training history available for plotting.\")\n",
    "        return\n",
    "        \n",
    "    loss_data = history.history['loss']\n",
    "    val_loss_data = history.history.get('val_loss', [])\n",
    "    \n",
    "    final_train_loss = loss_data[-1]\n",
    "    final_val_loss = val_loss_data[-1] if val_loss_data else None\n",
    "    total_epochs = len(loss_data)\n",
    "\n",
    "    indices = range(0, len(loss_data), 3)\n",
    "    points = max(1, int(len(loss_data) * 0.05))\n",
    "\n",
    "    indices_train = list(range(0, len(loss_data), points))\n",
    "    indices_val = list(range(0, len(val_loss_data), points)) if val_loss_data else []\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    if val_loss_data:\n",
    "        plt.plot(\n",
    "            range(len(val_loss_data)),\n",
    "            val_loss_data,\n",
    "            linestyle='-',\n",
    "            color='darkgrey',\n",
    "            label=f'Validation Loss (Final: {final_val_loss:.3e})'\n",
    "        )\n",
    "        plt.plot(\n",
    "            indices_val,\n",
    "            [val_loss_data[i] for i in indices_val],\n",
    "            linestyle='',\n",
    "            color='royalblue',\n",
    "            marker='^',\n",
    "            markersize=10,\n",
    "            markerfacecolor='white',\n",
    "            markeredgecolor='grey',\n",
    "            markeredgewidth=2\n",
    "        )\n",
    "\n",
    "    plt.plot(\n",
    "        indices,\n",
    "        [loss_data[i] for i in indices],\n",
    "        linestyle='-',\n",
    "        color='royalblue',\n",
    "        linewidth=1.5,\n",
    "        marker='',\n",
    "        markersize=12,\n",
    "        markeredgecolor='purple',\n",
    "        label=f'Training Loss (Final: {final_train_loss:.3e})'\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        indices_train,\n",
    "        [loss_data[i] for i in indices_train],\n",
    "        linestyle='',\n",
    "        color='royalblue',\n",
    "        marker='o',\n",
    "        markersize=10,\n",
    "        markerfacecolor='white',\n",
    "        markeredgecolor='purple',\n",
    "        markeredgewidth=2\n",
    "    )\n",
    "\n",
    "    # Add termination condition information\n",
    "    title = fr'DNN (PReLU) - Loss vs. Epoch for ({y.name})'\n",
    "    subtitle = f'Completed {total_epochs} epochs'\n",
    "    \n",
    "    # Add horizontal line for validation loss threshold\n",
    "    if val_loss_data:\n",
    "        plt.axhline(y=1e-6, color='red', linestyle='--', alpha=0.7, \n",
    "                   label='Val Loss Threshold (1e-6)')\n",
    "\n",
    "    plt.title(title, fontsize=24, fontweight='bold')\n",
    "    plt.text(0.5, 0.95, subtitle, transform=plt.gca().transAxes, \n",
    "             fontsize=14, ha='center', va='top')\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=20, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=20, fontweight='bold')\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(save_file_path, savename + '_DNN_loss_vs_epoch.jpg'), \n",
    "                dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "################################################################\n",
    "# Function to perform 5-fold CV with the DNN predicted results\n",
    "################################################################\n",
    "def plot_5fold_cv(model, X, y):\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    r2_scores = []\n",
    "    mae_scores = []\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "        fold += 1\n",
    "    \n",
    "    # Plot\n",
    "    folds = np.arange(1, 6)\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(5.5, 4.5))\n",
    "\n",
    "    # Left axis - R\n",
    "    color_r2 = 'darkblue'\n",
    "    ax1.bar(folds - width/2, r2_scores, width, color=color_r2, edgecolor='black', label='R Score')\n",
    "    ax1.set_xlabel('Fold Number', fontsize=14)\n",
    "    ax1.set_ylabel('R Score', fontsize=14, color=color_r2)\n",
    "    ax1.tick_params(axis='y', labelcolor=color_r2)\n",
    "\n",
    "    # Right axis - MAE\n",
    "    ax2 = ax1.twinx()\n",
    "    color_mae = 'darkred'\n",
    "    ax2.bar(folds + width/2, mae_scores, width, color=color_mae, edgecolor='black', label='MAE')\n",
    "    ax2.set_ylabel('Mean Absolute Error', fontsize=14, color=color_mae)\n",
    "    ax2.tick_params(axis='y', labelcolor=color_mae)\n",
    "\n",
    "    # Legends\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.title('5-Fold CV Performance', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "################################################################\n",
    "# Function to Save data for later optimization\n",
    "################################################################\n",
    "\n",
    "def save_model_with_metadata(model, scaler_X, scaler_y, save_file_path, savename, model_name, \n",
    "                             input_features, output_features, X_data, y_data):\n",
    "    save_folder = os.path.join(save_file_path, savename)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Compute min/max directly from data\n",
    "    X_min = X_data.min(axis=0).tolist()\n",
    "    X_max = X_data.max(axis=0).tolist()\n",
    "    y_min = y_data.min(axis=0).tolist() if y_data is not None else None\n",
    "    y_max = y_data.max(axis=0).tolist() if y_data is not None else None\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"model_name\": model_name,\n",
    "        \"scaler_X_path\": scaler_X_path,\n",
    "        \"scaler_y_path\": scaler_y_path,\n",
    "        \"input_features\": list(input_features),\n",
    "        \"output_features\": list(output_features),\n",
    "        \"scaler_X_mean\": scaler_X.mean_.tolist(),\n",
    "        \"scaler_X_scale\": scaler_X.scale_.tolist(),\n",
    "        \"scaler_y_mean\": scaler_y.mean_.tolist() if hasattr(scaler_y, \"mean_\") else None,\n",
    "        \"scaler_y_scale\": scaler_y.scale_.tolist() if hasattr(scaler_y, \"scale_\") else None,\n",
    "        \"scaler_X_min\": X_min,\n",
    "        \"scaler_X_max\": X_max,\n",
    "        \"scaler_y_min\": y_min,\n",
    "        \"scaler_y_max\": y_max,\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(save_folder, f\"{savename}_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(f\"Model, scalers, and metadata saved at: {save_folder}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_metadata_to_excel(save_file_path, savename, metadata_list):\n",
    "    \n",
    "    excel_path = os.path.join(save_file_path, savename, f\"{savename}_model_info.xlsx\")\n",
    "    \n",
    "    # Flatten metadata for Excel\n",
    "    rows = []\n",
    "    for md in metadata_list:\n",
    "        rows.append({\n",
    "            \"Model Name\": md[\"model_name\"],\n",
    "            \"Model Path\": md[\"model_path\"],\n",
    "            \"Scaler X Path\": md[\"scaler_X_path\"],\n",
    "            \"Scaler Y Path\": md[\"scaler_y_path\"],\n",
    "            \"Input Features\": \", \".join(md[\"input_features\"]),\n",
    "            \"Output Features\": \", \".join(md[\"output_features\"]),\n",
    "            \"Scaler X Min\": md[\"scaler_X_min\"],\n",
    "            \"Scaler X Max\": md[\"scaler_X_max\"],\n",
    "            \"Scaler Y Min\": md[\"scaler_y_min\"],\n",
    "            \"Scaler Y Max\": md[\"scaler_y_max\"],\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f\"Metadata saved to Excel: {excel_path}\")\n",
    "\n",
    "###################\n",
    "# Model execution\n",
    "###################\n",
    "\n",
    "print(f\"{BLUE_TEXT} Starting DNN...{RESET_TEXT}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "start_time_total_DNN = time.time() \n",
    "\n",
    "# Building the deep neural network model\n",
    "model = build_model(X_train.shape[1])\n",
    "\n",
    "# Training with termination conditions\n",
    "history, elapsed_time_1 = compile_and_train(\n",
    "    model, X_train, y_train, X_test, y_test,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_wall_time_hours=2  # 2 hours maximum\n",
    ")\n",
    "\n",
    "# Continue with evaluation only if training was successful\n",
    "if history is not None:\n",
    "    # Calculate errors in the predictions from DNN\n",
    "    elapsed_time_2, elapsed_time_3, MAE_loss_train_DNN, MAE_loss_test_DNN, \\\n",
    "    rmse_train_DNN, rmse_test_DNN, R2_score_train_DNN, R2_score_test_DNN, \\\n",
    "    predicted_train, predicted_test = evaluate_model(model, X_test, y_test, X_train, y_train)\n",
    "\n",
    "    end_time_total_DNN = time.time()\n",
    "    elapsed_time_total_DNN = end_time_total_DNN - start_time_total_DNN\n",
    "\n",
    "    # Save the trained DNN model\n",
    "    save_model(model, scaler_X_NN, scaler_y_NN, save_file_path, 'trained_model')\n",
    "\n",
    "    # Save the scalers\n",
    "    joblib.dump(scaler_X_NN, os.path.join(save_file_path, savename, savename + '_scaler_X.joblib'))\n",
    "    joblib.dump(scaler_y_NN, os.path.join(save_file_path, savename, savename + '_scaler_y.joblib'))\n",
    "\n",
    "    # Save predictions from DNN\n",
    "    save_results_to_excel(y_train, predicted_train, y_test, predicted_test, \n",
    "                         MAE_loss_train_DNN, MAE_loss_test_DNN, rmse_train_DNN, rmse_test_DNN, \n",
    "                         R2_score_train_DNN, R2_score_test_DNN, 'Input_vs_Prediction', \n",
    "                         save_file_path, savename, scaler_X_NN, scaler_y_NN)\n",
    "\n",
    "    # Plot results\n",
    "    plot_results(elapsed_time_1, elapsed_time_2, elapsed_time_3, elapsed_time_total_DNN, \n",
    "                X_scaled, y_train, predicted_train, y_test, predicted_test, \n",
    "                MAE_loss_train_DNN, MAE_loss_test_DNN, rmse_train_DNN, rmse_test_DNN, \n",
    "                R2_score_train_DNN, R2_score_test_DNN, scaler_X_NN, scaler_y_NN, \n",
    "                'Input_vs_Prediction', savename)\n",
    "\n",
    "    # Plot loss curve\n",
    "    plot_loss(history, savename)\n",
    "\n",
    "    # Perform 5-fold CV\n",
    "    plot_5fold_cv(model, X_scaled, y_scaled)\n",
    "\n",
    "    # Saving model in json format\n",
    "    metadata_list = []\n",
    "\n",
    "    # Save model + scalers + metadata\n",
    "    save_model_with_metadata(\n",
    "        model, \n",
    "        scaler_X_NN, \n",
    "        scaler_y_NN, \n",
    "        save_file_path,\n",
    "        savename,\n",
    "        model_name=\"DeepModel\",\n",
    "        input_features=X.columns, \n",
    "        output_features=[y.name],\n",
    "        X_data=X.values, \n",
    "        y_data=y.values\n",
    "        )    \n",
    "\n",
    "\n",
    "    # Load the saved metadata json for Excel export\n",
    "    with open(os.path.join(save_file_path, savename, f\"{savename}_metadata.json\")) as f:\n",
    "        metadata = json.load(f)\n",
    "    metadata_list.append(metadata)\n",
    "\n",
    "    # Save all metadata to Excel\n",
    "    save_metadata_to_excel(save_file_path, savename, metadata_list)\n",
    "\n",
    "    print(f\"{GREEN_TEXT}DNN training completed successfully!{RESET_TEXT}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"{RED_TEXT}Training failed. Please check the error messages above.{RESET_TEXT}\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "elapsed_time_total = end_time_total - start_time_total\n",
    "\n",
    "print(f\"{BLUE_TEXT}\\nTotal runtime: {elapsed_time_total:.2f} seconds{RESET_TEXT}\")\n",
    "print(f\"{GREEN_TEXT}Completed{RESET_TEXT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
